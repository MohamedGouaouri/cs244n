{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gensim_word_vectors.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Gensim Word Vectors"
      ],
      "metadata": {
        "id": "8zAvSUBwSbgl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/text/blob/master/docs/guide/word_embeddings.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
        "    Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/text/docs/guide/word_embeddings.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ],
      "metadata": {
        "id": "auitfybIa0uL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Get the interactive Tools for Matplotlib\n",
        "%matplotlib notebook\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec"
      ],
      "metadata": {
        "id": "oYFIky4-SdnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is Gensim\n",
        "For looking at word vectors, W'll use Gensim. \\\\\n",
        "Gensim isn't really a deep learning package. It's a package for for word and text similarity modeling, which started with (LDA-style) topic models and grew into SVD and neural word representations. But its efficient and scalable, and quite widely used.\\\\\n",
        "\n",
        "## Stanford GloVe model\n",
        "Our homegrown Stanford offering is GloVe word vectors. Gensim doesn't give them first class support, but allows you to convert a file of GloVe vectors into word2vec format. You can download the GloVe vectors from [the Glove page](https://nlp.stanford.edu/projects/glove/). They're inside [this zip file](https://nlp.stanford.edu/data/glove.6B.zip)\n",
        "\n"
      ],
      "metadata": {
        "id": "Q8DS4foaSsiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "919qBRouTFT3",
        "outputId": "783a8888-a3c3-4980-90db-3cf9bb39dd03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-08-05 09:18:43--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2022-08-05 09:18:44--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.04MB/s    in 2m 42s  \n",
            "\n",
            "2022-08-05 09:21:27 (5.07 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glove_file = \"./glove.6B.300d.txt\"\n",
        "word2vec_glove_file = \"glove.6B.300d.word2vec.txt\"\n",
        "glove2word2vec(glove_file, word2vec_glove_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRCIXldoTMJG",
        "outputId": "b50409b0-b164-4cb0-95ff-408680a330a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(400000, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = KeyedVectors.load_word2vec_format(word2vec_glove_file)"
      ],
      "metadata": {
        "id": "0UhC7l4sUr_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.most_similar(\"arabic\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlCqNp8wVCZ1",
        "outputId": "44599abd-27ff-464c-ed91-38615f34f79d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('hebrew', 0.6557009220123291),\n",
              " ('language', 0.6296008825302124),\n",
              " ('urdu', 0.5955149531364441),\n",
              " ('languages', 0.5669240355491638),\n",
              " ('hindi', 0.5659825801849365),\n",
              " ('farsi', 0.5628933906555176),\n",
              " ('persian', 0.5627796649932861),\n",
              " ('english', 0.5513119697570801),\n",
              " ('word', 0.5412472486495972),\n",
              " ('pashto', 0.5407745838165283)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define functions to write word vectors to file inorder to visualize them using the [embeddings projector](http://projector.tensorflow.org)"
      ],
      "metadata": {
        "id": "g7u8XrvEZBV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_words(model, max_vocab_size=10000):\n",
        "    i = 1\n",
        "    with open(\"vocab.txt\", \"w\") as vocab_file:\n",
        "        for word, _ in model.vocab.items():\n",
        "            if i <= max_vocab_size:\n",
        "                vocab_file.write(word)\n",
        "                vocab_file.write(\"\\n\")\n",
        "            else:\n",
        "                break\n",
        "            i += 1\n",
        "        \n",
        "def save_vectors(model, max_vocab_size=10000):\n",
        "    i = 1\n",
        "    with open(\"vectors.txt\", \"w\") as vectors_file:\n",
        "        for word, _ in model.vocab.items():\n",
        "            if i <= max_vocab_size:\n",
        "                vector = model.get_vector(word)\n",
        "                for weight in vector:\n",
        "                    vectors_file.write(f\"{weight}\\t\")\n",
        "                vectors_file.write(\"\\n\")\n",
        "            else:\n",
        "                break\n",
        "            i += 1"
      ],
      "metadata": {
        "id": "DTi982ZyV2ip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_words(model)"
      ],
      "metadata": {
        "id": "K3GKPon3V8fQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_vectors(model)"
      ],
      "metadata": {
        "id": "L6zebaBrWwb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can word embeddings to find analogy\n",
        "def analogy(w1, w2, y):\n",
        "    return model.most_similar(positive=[y, w2], negative=[w1])[0][0]"
      ],
      "metadata": {
        "id": "l_6Yy9k5ZgKy"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analogy('japan', 'japanese', 'australia')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fjJm4ozebfh-",
        "outputId": "6649306b-6052-404b-8625-0916f80d525e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'australian'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QIWy4Nizbh7O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}